{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48529b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import optuna\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2896b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-13 10:31:27,364]\u001b[0m A new study created in memory with name: no-name-8fada6b9-c714-450e-b57c-ca9718c7f892\u001b[0m\n",
      "\u001b[32m[I 2023-04-13 10:31:28,926]\u001b[0m Trial 0 finished with value: 0.34609375 and parameters: {'n_layers': 2, 'n_units_l0': 89, 'dropout_l0': 0.2633559814619951, 'n_units_l1': 71, 'dropout_l1': 0.4650445570018686, 'optimizer': 'Adam', 'lr': 2.2408175149649754e-05}. Best is trial 0 with value: 0.34609375.\u001b[0m\n",
      "\u001b[32m[I 2023-04-13 10:31:30,498]\u001b[0m Trial 1 finished with value: 0.79453125 and parameters: {'n_layers': 2, 'n_units_l0': 119, 'dropout_l0': 0.3137396985993419, 'n_units_l1': 113, 'dropout_l1': 0.36903356884838157, 'optimizer': 'RMSprop', 'lr': 0.0005775031334583477}. Best is trial 1 with value: 0.79453125.\u001b[0m\n",
      "\u001b[32m[I 2023-04-13 10:31:32,065]\u001b[0m Trial 2 finished with value: 0.52578125 and parameters: {'n_layers': 3, 'n_units_l0': 40, 'dropout_l0': 0.421954222702848, 'n_units_l1': 32, 'dropout_l1': 0.23982457230171117, 'n_units_l2': 61, 'dropout_l2': 0.20212472871566745, 'optimizer': 'SGD', 'lr': 0.03974513992029768}. Best is trial 1 with value: 0.79453125.\u001b[0m\n",
      "\u001b[32m[I 2023-04-13 10:31:33,595]\u001b[0m Trial 3 finished with value: 0.81875 and parameters: {'n_layers': 1, 'n_units_l0': 116, 'dropout_l0': 0.30978549968633906, 'optimizer': 'RMSprop', 'lr': 0.005794355936432765}. Best is trial 3 with value: 0.81875.\u001b[0m\n",
      "\u001b[32m[I 2023-04-13 10:31:35,193]\u001b[0m Trial 4 finished with value: 0.1453125 and parameters: {'n_layers': 3, 'n_units_l0': 15, 'dropout_l0': 0.4556170477200105, 'n_units_l1': 22, 'dropout_l1': 0.3540704012134172, 'n_units_l2': 75, 'dropout_l2': 0.36805823296930007, 'optimizer': 'Adam', 'lr': 1.2635343856516984e-05}. Best is trial 3 with value: 0.81875.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  5\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  5\n",
      "Best trial:\n",
      "  Value:  0.81875\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_l0: 116\n",
      "    dropout_l0: 0.30978549968633906\n",
      "    optimizer: RMSprop\n",
      "    lr: 0.005794355936432765\n"
     ]
    }
   ],
   "source": [
    "# Derived partially from \n",
    "# - https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n",
    "# - https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 10\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 10\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "\n",
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 28 * 28\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def get_mnist():\n",
    "    # Load FashionMNIST dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = define_model(trial).to(device)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=5, timeout=600)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b25148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
