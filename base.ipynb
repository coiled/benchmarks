{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44900f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n"
     ]
    }
   ],
   "source": [
    "# Derived partially from \n",
    "# - https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n",
    "# - https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "import os\n",
    "import optuna\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"data/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# We create a fake image dataset. This ought to be replaced by\n",
    "# your actual dataset or pytorch's example datasets. We do this here\n",
    "# to focus more on computation than actual convergence.\n",
    "class FakeImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, count=1_000):\n",
    "        self.labels = np.random.randint(0, 5, size=count)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        torch.random.seed = label\n",
    "        img = torch.rand(3, image_size, image_size)\n",
    "        return img, label\n",
    "\n",
    "dataset = FakeImageDataset()\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, activation=nn.Sigmoid()):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            activation\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "    @classmethod\n",
    "    def from_trial(cls, trial):\n",
    "        activations = [nn.Sigmoid, nn.Tanh]\n",
    "        idx = trial.suggest_categorical(\"generator_activation\", list(range(len(activations))))\n",
    "        activation = activations[idx]()\n",
    "        return cls(ngpu, activation)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, leaky_relu_slope=0.2, activation=nn.Sigmoid()):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(leaky_relu_slope, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(leaky_relu_slope, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(leaky_relu_slope, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(leaky_relu_slope, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            activation\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_trial(cls, trial):\n",
    "        activations = [nn.Sigmoid, nn.Tanh]\n",
    "        idx = trial.suggest_categorical(\"descriminator_activation\", list(range(len(activations))))\n",
    "        activation = activations[idx]()\n",
    "        \n",
    "        slopes = np.arange(0.1, 0.4, 0.1)\n",
    "        idx = trial.suggest_categorical(\"descriminator_leaky_relu_slope\", list(range(len(slopes))))\n",
    "        leaky_relu_slope = slopes[idx]\n",
    "        return cls(ngpu, leaky_relu_slope, activation)\n",
    "        \n",
    "\n",
    "def objective(trial):\n",
    "    ############################\n",
    "    ### Create the generator ###\n",
    "    netG = Generator.from_trial(trial).to(device)\n",
    "\n",
    "    # Handle multi-GPU if desired\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "    # Apply the ``weights_init`` function to randomly initialize all weights\n",
    "    #  to ``mean=0``, ``stdev=0.02``.\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "    ################################\n",
    "    ### Create the Discriminator ###\n",
    "    netD = Discriminator.from_trial(trial).to(device)\n",
    "\n",
    "    # Handle multi-GPU if desired\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "    # Apply the ``weights_init`` function to randomly initialize all weights\n",
    "    # like this: ``to mean=0, stdev=0.2``.\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    ############################################\n",
    "    ### Remaining crierion, optimizers, etc. ###\n",
    "    # Initialize the ``BCELoss`` function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Create batch of latent vectors that we will use to visualize\n",
    "    #  the progression of the generator\n",
    "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "    # Establish convention for real and fake labels during training\n",
    "    real_label = 1.\n",
    "    fake_label = 0.\n",
    "    \n",
    "    # Learning rate for optimizers\n",
    "    lr = 0.00001\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    \n",
    "    #####################\n",
    "    ### Training Loop ###\n",
    "\n",
    "    # Lists to keep track of progress\n",
    "    img_list = []\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    iters = 0\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            # Format batch\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            # Classify all fake batch with D\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label)\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Output training stats\n",
    "            if i % 50 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake = netG(fixed_noise).detach().cpu()\n",
    "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "        # Report to Optuna\n",
    "        trial.report(errD.item(), epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    return errD.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8135dad2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-13 13:07:36,513]\u001b[0m A new study created in memory with name: no-name-0adfaa9c-e253-49e9-b438-b1a97eab8ec2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/20][0/8]\tLoss_D: 2.5841\tLoss_G: 0.3823\tD(x): 0.5959\tD(G(z)): 0.7853 / 0.7218\n",
      "[1/20][0/8]\tLoss_D: 2.0248\tLoss_G: 0.5390\tD(x): 0.6194\tD(G(z)): 0.6994 / 0.6256\n",
      "[2/20][0/8]\tLoss_D: 1.6571\tLoss_G: 0.7571\tD(x): 0.6277\tD(G(z)): 0.6065 / 0.5287\n",
      "[3/20][0/8]\tLoss_D: 1.5882\tLoss_G: 0.8132\tD(x): 0.6352\tD(G(z)): 0.5752 / 0.4980\n",
      "[4/20][0/8]\tLoss_D: 1.2252\tLoss_G: 1.0388\tD(x): 0.6942\tD(G(z)): 0.4903 / 0.4162\n",
      "[5/20][0/8]\tLoss_D: 1.1717\tLoss_G: 1.1336\tD(x): 0.6740\tD(G(z)): 0.4562 / 0.3864\n",
      "[6/20][0/8]\tLoss_D: 1.0674\tLoss_G: 1.0875\tD(x): 0.7344\tD(G(z)): 0.4648 / 0.3870\n",
      "[7/20][0/8]\tLoss_D: 0.9387\tLoss_G: 1.2400\tD(x): 0.7507\tD(G(z)): 0.4139 / 0.3432\n",
      "[8/20][0/8]\tLoss_D: 0.9314\tLoss_G: 1.2585\tD(x): 0.7622\tD(G(z)): 0.4172 / 0.3477\n",
      "[9/20][0/8]\tLoss_D: 0.8676\tLoss_G: 1.2463\tD(x): 0.7957\tD(G(z)): 0.4142 / 0.3421\n",
      "[10/20][0/8]\tLoss_D: 0.8796\tLoss_G: 1.3269\tD(x): 0.7734\tD(G(z)): 0.3916 / 0.3225\n",
      "[11/20][0/8]\tLoss_D: 0.8009\tLoss_G: 1.4135\tD(x): 0.7884\tD(G(z)): 0.3729 / 0.3059\n",
      "[12/20][0/8]\tLoss_D: 0.8263\tLoss_G: 1.3633\tD(x): 0.8020\tD(G(z)): 0.3904 / 0.3165\n",
      "[13/20][0/8]\tLoss_D: 0.7557\tLoss_G: 1.4855\tD(x): 0.7888\tD(G(z)): 0.3480 / 0.2852\n",
      "[14/20][0/8]\tLoss_D: 0.7484\tLoss_G: 1.4800\tD(x): 0.8051\tD(G(z)): 0.3568 / 0.2865\n",
      "[15/20][0/8]\tLoss_D: 0.8153\tLoss_G: 1.5202\tD(x): 0.7503\tD(G(z)): 0.3474 / 0.2808\n",
      "[16/20][0/8]\tLoss_D: 0.7602\tLoss_G: 1.4994\tD(x): 0.7652\tD(G(z)): 0.3358 / 0.2705\n",
      "[17/20][0/8]\tLoss_D: 0.8132\tLoss_G: 1.4568\tD(x): 0.7674\tD(G(z)): 0.3626 / 0.2883\n",
      "[18/20][0/8]\tLoss_D: 0.7503\tLoss_G: 1.4986\tD(x): 0.7763\tD(G(z)): 0.3429 / 0.2753\n",
      "[19/20][0/8]\tLoss_D: 0.8392\tLoss_G: 1.4541\tD(x): 0.7304\tD(G(z)): 0.3523 / 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-13 13:07:49,475]\u001b[0m Trial 0 finished with value: 0.8317961692810059 and parameters: {'generator_activation': 1, 'descriminator_activation': 0, 'descriminator_leaky_relu_slope': 3}. Best is trial 0 with value: 0.8317961692810059.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765b9332",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Timed out trying to connect to tcp://cluster-skybt.dask.host:8686 after 30 s",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/comm/tcp.py:498\u001b[0m, in \u001b[0;36mBaseTCPConnector.connect\u001b[0;34m(self, address, deserialize, **connection_args)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[1;32m    499\u001b[0m         ip, port, max_buffer_size\u001b[38;5;241m=\u001b[39mMAX_BUFFER_SIZE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Under certain circumstances tornado will have a closed connection with an\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# error and not raise a StreamClosedError.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# This occurs with tornado 5.x and openssl 1.1+\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/tornado/tcpclient.py:275\u001b[0m, in \u001b[0;36mTCPClient.connect\u001b[0;34m(self, host, port, af, ssl_options, max_buffer_size, source_ip, source_port, timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m connector \u001b[38;5;241m=\u001b[39m _Connector(\n\u001b[1;32m    267\u001b[0m     addrinfo,\n\u001b[1;32m    268\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m     ),\n\u001b[1;32m    274\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m af, addr, stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connector\u001b[38;5;241m.\u001b[39mstart(connect_timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# TODO: For better performance we could cache the (af, addr)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# information here and re-use it on subsequent connections to\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# the same host. (http://tools.ietf.org/html/rfc6555#section-4.2)\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/asyncio/tasks.py:456\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/comm/core.py:291\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m    292\u001b[0m         connector\u001b[38;5;241m.\u001b[39mconnect(loc, deserialize\u001b[38;5;241m=\u001b[39mdeserialize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconnection_args),\n\u001b[1;32m    293\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(intermediate_cap, time_left()),\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/asyncio/tasks.py:458\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 458\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtcp://cluster-skybt.dask.host:8686\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/client.py:988\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/client.py:1185\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/utils.py:406\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    405\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/utils.py:379\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    378\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 379\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/tornado/gen.py:762\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/client.py:1265\u001b[0m, in \u001b[0;36mClient._start\u001b[0;34m(self, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_connected(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close()\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/client.py:1328\u001b[0m, in \u001b[0;36mClient._ensure_connected\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connecting_to_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1328\u001b[0m     comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connect(\n\u001b[1;32m   1329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39maddress, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_args\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m     comm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient->Scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/coiled-benchmarks/lib/python3.10/site-packages/distributed/comm/core.py:317\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(backoff)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out trying to connect to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mactive_exception\u001b[39;00m\n\u001b[1;32m    321\u001b[0m local_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomm\u001b[38;5;241m.\u001b[39mhandshake_info(),\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(handshake_overrides \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    324\u001b[0m }\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# This would be better, but connections leak if worker is closed quickly\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# write, handshake = await asyncio.gather(comm.write(local_info), comm.read())\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Timed out trying to connect to tcp://cluster-skybt.dask.host:8686 after 30 s"
     ]
    }
   ],
   "source": [
    "from distributed import Client\n",
    "client = Client('tcp://cluster-skybt.dask.host:8686')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
